{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 音声認識・対話技術講習会　音源分離の基礎と最新動向　演習資料\n",
        "- 本演習では，[pyroomacoustics](https://github.com/LCAV/pyroomacoustics)を用いたブラインド音源分離（AuxIVA，ILRMA）を行います．\n",
        "- 本演習のコードは，pyroomacousticsのデモ用notebookを基に作成されています．\n",
        "\n",
        "Created by Tomohiko Nakamura and Hiroshi Saruwatari"
      ],
      "metadata": {
        "id": "P9kAc8xmQGMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 環境設定"
      ],
      "metadata": {
        "id": "ZCt4BlurRE94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "必要なライブラリをインストール"
      ],
      "metadata": {
        "id": "l_oi5U8sSv9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHxaWHdn7xSj"
      },
      "outputs": [],
      "source": [
        "!pip install pyroomacoustics==0.7.3 librosa matplotlib numpy==1.24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ライブラリをインポート"
      ],
      "metadata": {
        "id": "ZK6vgnLojwt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import librosa\n",
        "import soundfile\n",
        "import IPython.display\n",
        "import pyroomacoustics\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display"
      ],
      "metadata": {
        "id": "cLg2ltwXCbn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "音サンプルファイルをダウンロード"
      ],
      "metadata": {
        "id": "dkl1_6zDuETI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --quiet -O sample_jvs001.wav \"https://drive.google.com/uc?export=download&id=142aj-qFJOhoteWKqgRzvNoq02JbZIsaG\"\n",
        "!wget --quiet -O sample_jvs002.wav \"https://drive.google.com/uc?export=download&id=1idCghceyP9HldFnBKKx9_2ENqXWnr7IP\"\n",
        "!wget --quiet -O sample_jvs003.wav \"https://drive.google.com/uc?export=download&id=1plvIsG5Y0l-lYYAMIBH8YHRkDr_prwLM\"\n",
        "!wget --quiet -O sample_pjs001.wav \"https://drive.google.com/uc?export=download&id=1rvkjFYViRer120UiAa2rtveDzbhtNEAN\"\n",
        "!wget --quiet -O sample_trijek001_en.wav \"https://drive.google.com/uc?export=download&id=1JdUU2CTTxHZHnftoUh12T7Ozggc_SQ-B\""
      ],
      "metadata": {
        "id": "KHxpvke3uBR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 音ファイルの読み込み"
      ],
      "metadata": {
        "id": "X0Ln68CYRKKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "音ファイル読み込み用関数を定義"
      ],
      "metadata": {
        "id": "dFYQ9yJ0RXm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio_file(filename, target_sample_rate, max_duration=20.0):\n",
        "  waveform, org_sample_rate = soundfile.read(filename, stop=int(soundfile.info(filename).samplerate*max_duration), always_2d=True)\n",
        "  waveform = waveform.mean(axis=1)\n",
        "  waveform = librosa.resample(y=waveform.T, orig_sr=org_sample_rate, target_sr=target_sample_rate).T\n",
        "  return waveform"
      ],
      "metadata": {
        "id": "hvWhRAOFEteS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "利用可能な音ファイル"
      ],
      "metadata": {
        "id": "T852vrgPJsOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *.wav"
      ],
      "metadata": {
        "id": "3L1YMTvcJpKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "対象となる音ファイルを指定し，読み込み"
      ],
      "metadata": {
        "id": "pe_XtshaRaZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate = 16000\n",
        "\n",
        "waveforms = [\n",
        "    load_audio_file(librosa.example(\"libri1\"), sample_rate).astype('f'),\n",
        "    load_audio_file(librosa.example(\"libri2\"), sample_rate).astype('f'),\n",
        "]\n",
        "\n",
        "# JVSのサンプル音で実行する場合はこちら\n",
        "# waveforms = [\n",
        "#     load_audio_file(\"sample_jvs002.wav\", sample_rate).astype('f'),\n",
        "#     load_audio_file(\"sample_jvs001.wav\", sample_rate).astype('f'),\n",
        "# ]"
      ],
      "metadata": {
        "id": "AaQHu0fo8VFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## シミュレーションによる混合音作成"
      ],
      "metadata": {
        "id": "ZUJ8prU6HJ4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "部屋，音源，マイクの位置を設定"
      ],
      "metadata": {
        "id": "c-jgIsSEKZiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 部屋の大きさ（x軸，y軸）．原点は(0,0)．\n",
        "room_dim = [8, 9]\n",
        "\n",
        "# 音源位置\n",
        "src_locations = [\n",
        "    [2.5, 6.0], # Src 1\n",
        "    [2.5, 3.0], # Src 2\n",
        "]\n",
        "# 音源再生時の時間遅れ [s]\n",
        "delays = [1.0, 0.0]\n",
        "\n",
        "# マイク位置\n",
        "mic_locations = numpy.c_[\n",
        "    [7, 4.495], # Mic 1\n",
        "    [7, 4.505], # Mic 2\n",
        "]\n",
        "\n",
        "# 部屋を作成\n",
        "room = pyroomacoustics.ShoeBox(room_dim, fs=sample_rate, max_order=15, absorption=0.35, sigma2_awgn=1e-8)\n",
        "\n",
        "# 部屋への音源の登録と，音源再生時の時間遅れの付与\n",
        "for sig, d, loc in zip(waveforms, delays, src_locations):\n",
        "    room.add_source(loc, signal=sig, delay=d)\n",
        "\n",
        "# 部屋へのマイクアレイの登録\n",
        "room.add_microphone_array(pyroomacoustics.MicrophoneArray(mic_locations, room.fs))\n",
        "\n",
        "# 部屋，音源，マイクの図示\n",
        "fig, ax = room.plot()\n",
        "ax.set_xlabel(\"x [m]\")\n",
        "ax.set_ylabel(\"y [m]\")\n",
        "ax.set_box_aspect(1)\n",
        "ax.set_xlim([-0.5, 0.5+room_dim[0]])\n",
        "ax.set_ylim([-0.5, 0.5+room_dim[1]])\n",
        "ax.legend([f\"Mic {m}\" for m in range(mic_locations.shape[1])]+[f\"Src {n}\" for n in range(len(src_locations))], ncol=4, loc=\"upper left\", bbox_to_anchor=(-0.05, 1.1), fancybox=False, frameon=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uj-E7EpzFc99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "正解音源と混合音の作成"
      ],
      "metadata": {
        "id": "Pwe42N-4T6Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解音源の作成（各マイクで各音源のみを再生したときの音響信号）\n",
        "separate_recordings = room.simulate(return_premix=True) # 音源数 x マイク数 x 時間\n",
        "\n",
        "# 混合音の作成\n",
        "observed_signals = numpy.sum(separate_recordings, axis=0) # マイク数 x 時間\n",
        "\n",
        "for m in range(observed_signals.shape[0]):\n",
        "  print(f\"Mixture signal: Channel {m}\")\n",
        "  IPython.display.display(IPython.display.Audio(observed_signals[m,:], rate=room.fs))"
      ],
      "metadata": {
        "id": "7IhCu-_0Hu4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AuxIVAとILRMAの実行"
      ],
      "metadata": {
        "id": "IN36Fz0LUDQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "観測音に対する短時間Fourier変換（STFT）の実行"
      ],
      "metadata": {
        "id": "tc8J2BsOUH2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STFTのパラメータ\n",
        "window_length = 2048\n",
        "hopsize = window_length // 4\n",
        "analysis_window = pyroomacoustics.windows.hann(window_length)\n",
        "synthesis_window = pyroomacoustics.transform.stft.compute_synthesis_window(analysis_window, hopsize)\n",
        "\n",
        "# 観測STFT\n",
        "X = pyroomacoustics.transform.stft.analysis(observed_signals.T, window_length, hopsize, win=analysis_window)"
      ],
      "metadata": {
        "id": "0BeP5p-4IRq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# スペクトログラム表示用クラス\n",
        "class SpectrogramVisualizer:\n",
        "  def __init__(self, sample_rate, window_length, hopsize, fig_size=(10,3)):\n",
        "    self.sample_rate = sample_rate\n",
        "    self.window_length = window_length\n",
        "    self.hopsize = hopsize\n",
        "    self.reset()\n",
        "    self.fig_size = fig_size\n",
        "\n",
        "  def reset(self):\n",
        "    self.iteration_counter = 0\n",
        "\n",
        "  def __call__(self, Y, title=\"Separated\", draw_iteration_number=True):\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(*self.fig_size)\n",
        "    for n in range(Y.shape[2]):\n",
        "      plt.subplot(1, Y.shape[2], n+1)\n",
        "      librosa.display.specshow(\n",
        "          librosa.amplitude_to_db(numpy.abs(Y[:,:,n].T), ref=numpy.max), y_axis=\"linear\", x_axis=\"time\",\n",
        "          sr=self.sample_rate, win_length=self.window_length, hop_length=self.hopsize\n",
        "        )\n",
        "      plt.xlabel(\"Time [s]\", fontsize=13)\n",
        "      if n == 0:\n",
        "        plt.ylabel(\"Frequency [Hz]\", fontsize=13)\n",
        "      else:\n",
        "        plt.ylabel(\"\")\n",
        "      plt.title(f'{title} {n+1} (Iteration {self.iteration_counter})' if draw_iteration_number else f'{title} {n+1}')\n",
        "    plt.show(block=False)\n",
        "    self.iteration_counter += 10\n",
        "\n",
        "visualizer = SpectrogramVisualizer(room.fs, window_length=window_length, hopsize=hopsize)"
      ],
      "metadata": {
        "id": "Ol1MTbVHcvTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AuxIVAの実行と逆STFTの適用"
      ],
      "metadata": {
        "id": "nH54cN7dUOso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer.reset()\n",
        "\n",
        "# AuxIVAを実行\n",
        "auxiva_Y = pyroomacoustics.bss.auxiva(X, n_iter=50, proj_back=True, callback=visualizer)\n",
        "visualizer(auxiva_Y)\n",
        "\n",
        "# iSTFTで分離信号を取得\n",
        "auxiva_y = pyroomacoustics.transform.stft.synthesis(auxiva_Y, window_length, hopsize, win=synthesis_window)\n",
        "auxiva_y = auxiva_y[window_length - hopsize:, :].T"
      ],
      "metadata": {
        "id": "0n6X1CPkMbcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ILRMAの実行"
      ],
      "metadata": {
        "id": "lDuc-fOUUTOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer.reset()\n",
        "\n",
        "# ILRMAの実行\n",
        "ilrma_Y = pyroomacoustics.bss.ilrma(X, n_iter=50, n_components=10, proj_back=True, callback=visualizer)\n",
        "visualizer(ilrma_Y)\n",
        "\n",
        "# iSTFTで分離信号を取得\n",
        "ilrma_y = pyroomacoustics.transform.stft.synthesis(ilrma_Y, window_length, hopsize, win=synthesis_window)\n",
        "ilrma_y = ilrma_y[window_length - hopsize:, :].T"
      ],
      "metadata": {
        "id": "vFhn0OMjT0Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "正解音，分離音のスペクトログラムを表示"
      ],
      "metadata": {
        "id": "RxB6uKRTWJGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(pyroomacoustics.transform.stft.analysis(separate_recordings[:,0,:].T, window_length, hopsize, win=analysis_window), title=\"Groundtruth\", draw_iteration_number=False)\n",
        "visualizer(auxiva_Y, title=\"AuxIVA-separated\", draw_iteration_number=False)\n",
        "visualizer(ilrma_Y, title=\"ILRMA-separated\", draw_iteration_number=False)"
      ],
      "metadata": {
        "id": "NjlP8r4zNeYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "観測音，分離音，正解音の聴取"
      ],
      "metadata": {
        "id": "vfiV1DlkYWp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target_src_indexを1にすると，もう一方の音源を聴けます．\n",
        "target_src_index = 0\n",
        "\n",
        "print(\"Mixture signal (reference mic)\")\n",
        "IPython.display.display(IPython.display.Audio(observed_signals[target_src_index], rate=room.fs))\n",
        "\n",
        "print(\"Groundtruth signal\")\n",
        "IPython.display.display(IPython.display.Audio(separate_recordings[target_src_index,0,:], rate=room.fs))\n",
        "\n",
        "print(\"Separated signal (AuxIVA)\")\n",
        "IPython.display.display(IPython.display.Audio(auxiva_y[target_src_index,:], rate=room.fs))\n",
        "\n",
        "print(\"Separated signal (ILRMA)\")\n",
        "IPython.display.display(IPython.display.Audio(ilrma_y[target_src_index,:], rate=room.fs))\n"
      ],
      "metadata": {
        "id": "HOQob0LFOGK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJMZDV9tFgFz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}